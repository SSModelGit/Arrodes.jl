# construct a belief state out of the mdp parameters
beliefstate_for_pomdp(mdp::KAgentPOMDP) = KAgentBeliefUpdater(state_dims=length(mdp.start), env_dims=mdp.menv.M)
# since copy(::MCTSSolver) isn't defined for some reason, just use this instead of figuring out a copy extension
van_solver(params) = MCTSSolver(n_iterations=params[1], depth=20, exploration_constant=params[2])
dpw_solver(params) = DPWSolver(n_iterations=params[1], depth=20, exploration_constant=params[2])
function mcts_solver(pomdp::KAgentPOMDP; solver_params=[:van, 1000, 10.0])
    pomdp_bup = beliefstate_for_pomdp(pomdp)
    ğ’®_mcts = @match solver_params[1] begin
        :van => BeliefMCTSSolver(van_solver(solver_params[2:end]), pomdp_bup)
        :dpw => BeliefMCTSSolver(dpw_solver(solver_params[2:end]), pomdp_bup)
        _    => BeliefMCTSSolver(van_solver(solver_params[2:end]), pomdp_bup)
    end
    return ğ’®_mcts
end

# define Deep Q solver (REINFORCE, DQN, and SoftQ)
function deep_q_solver(pomdp::KAgentPOMDP; solver_params=[:all, 10000, 2, 512])
    as = actions(pomdp)
    S = state_space(pomdp)
    A() = Flux.gpu(DiscreteNetwork(Chain(Dense(Crux.dim(S)..., 64, relu), Dense(64, 64, relu), Dense(64, length(as))), as; dev=Flux.gpu))
    V() = Flux.gpu(ContinuousNetwork(Chain(Dense(Crux.dim(S)..., 64, relu), Dense(64, 64, relu), Dense(64, 1))))

    solver_type, N, epochs, batch_size = solver_params[[1, 2, 3, 4]] # can also do solver_params[1:2] if I wanted - keeping this as later reminder for self on how to use indexing
    ğ’®_net = @match solver_type begin
        :all => [REINFORCE(Ï€=A(), S=S, N=N, Î”N=500, a_opt=(epochs=epochs,), interaction_storage=[]),
                 DQN(Ï€=A(), S=S, N=N, interaction_storage=[]),
                 SoftQ(Ï€=A(), Î±=Float32(0.1), S=S, N=N, Î”N=1, c_opt=(;epochs=epochs, batch_size=batch_size), interaction_storage=[])]
        :reinforce => [REINFORCE(Ï€=A(), S=S, N=N, Î”N=500, a_opt=(epochs=epochs,), interaction_storage=[])]
        :dqn => [DQN(Ï€=A(), S=S, N=N, interaction_storage=[])]
        :softq => [SoftQ(Ï€=A(), Î±=Float32(0.1), S=S, N=N, Î”N=1, c_opt=(;epochs=epochs, batch_size=batch_size), interaction_storage=[])]
    end

    if !(solver_type==:all)
        return ğ’®_net[1]
    end
    return ğ’®_net
end

function solver_from_type(pomdp::KAgentPOMDP, type::Symbol=:dpw; solver_params)
    @match type begin
        :mcts => mcts_solver(pomdp;   solver_params=solver_params)
        :dql  => deep_q_solver(pomdp; solver_params=solver_params)
        _     => mcts_solver(pomdp;   solver_params=[:dpw, 1000, 10.0])
    end
end

function deep_q_metrics(pomdp::KAgentPOMDP, ğ’®_net; solver_type::Symbol=:all)
    @time Ï€_net = @match solver_type begin
        :all => map(x->solve(x, pomdp), ğ’®_net)
        _    => solve(ğ’®_net, pomdp)
    end
    labels = @match solver_type begin
        :all => ["REINFORCE", "DQN", "SoftQ" ]
        :reinforce => ["REINFORCE"]
        :dqn => ["DQN"]
        :softq => ["SoftQ" ]
    end
    p = plot_learning(ğ’®_net, title = "One-shot Agent Ï€ Training Curves", 
                        labels = labels)
    
    return Ï€_net, p
end

function quick_policy_compute_for_pomdp(pomdp::KAgentPOMDP; solver_type::Symbol=:mcts, solver_params=[:dpw, 1000, 10.0])
    ğ’®_pomdp = solver_from_type(pomdp, solver_type; solver_params=solver_params)
    Ï€_pomdp = solve(ğ’®_pomdp, pomdp)
    return ğ’®_pomdp, Ï€_pomdp
end

function quick_IQL(kworld::KWorld, anon_data::ExperienceBuffer; plot_metrics::Bool=false)
    N = anon_data.elements
    mdp = get_agent(kworld, "ag1")

    as = actions(mdp)
    S = state_space(mdp)
    Î³ = Float32(discount(mdp))
    A() = DiscreteNetwork(Chain(Dense(S.dims[1], 64, relu), Dense(64, 64, relu), Dense(64, length(as))), as; dev=Flux.cpu)

    ğ’Ÿ_iql = OnlineIQLearn(Ï€=A(), ğ’Ÿ_demo=anon_data, S=S, Î³=Î³, N=anon_data.elements, Î”N=1, c_opt=(;epochs=1),reg=false,gp=false, log=(;period=50))

    Ï€_iql = solve(ğ’Ÿ_iql, mdp)

    if plot_metrics; f = plot_learning([ğ’Ÿ_iql,], title="Results of IQ-Learning", labels=["iql",]); else; f = nothing; end
    return Ï€_iql, ğ’Ÿ_iql, mdp, f
end

function quick_IQL(mdp::KAgentPOMDP, anon_data::ExperienceBuffer)
    N = anon_data.elements
    as = actions(mdp)
    S = state_space(mdp)
    Î³ = Float32(discount(mdp))

    A() = DiscreteNetwork(Chain(Dense(S.dims[1], 64, relu), Dense(64, 64, relu), Dense(64, length(as))), as; dev=Flux.cpu)

    ğ’Ÿ_iql = OnlineIQLearn(Ï€=A(), ğ’Ÿ_demo=anon_data, S=S, Î³=Î³, N=anon_data.elements, Î”N=1, c_opt=(;epochs=1),reg=false,gp=false, log=(;period=50))

    Ï€_iql = solve(ğ’Ÿ_iql, mdp)

    return Ï€_iql, ğ’Ÿ_iql, mdp
end